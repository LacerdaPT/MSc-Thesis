\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Methods}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Supervised Learning}{1}}
\newlabel{subsec:supervised-learning}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Linear Regression}{2}}
\newlabel{subsubsec:linear-regression}{{1.1.1}{2}}
\newlabel{eq:MSE-definition}{{2}{2}}
\newlabel{eq:MSE-gradient}{{4}{3}}
\newlabel{eq:GD-update-rule}{{6}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Logistic Regression}{3}}
\newlabel{subsubsec:Logistic-Regression}{{1.1.2}{3}}
\newlabel{def:logistic-regression}{{7}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Artificial Neural Networks}{4}}
\newlabel{subsec:ANN}{{1.2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graphical visualization of an illustrative example of an artificial neural network. This ANN is composed by three layers with three neurons on the first two and one neuron on the output layer. The connections between all the neurons are also represented, in addition to the connection to the bias term represented by the extra nodes on the bottom with the label "+1". }}{5}}
\newlabel{fig:first-ANN}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Deep Learning}{7}}
\newlabel{subsec:Deep-Learning}{{1.3}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Stochastic Gradient Descent}{7}}
\newlabel{subsubsec:SGD}{{1.3.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}AdaGrad}{8}}
\newlabel{subsubsec:Adagrad}{{1.3.2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Parameter Initialization}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Loss Function}{9}}
\newlabel{subsubsec:loss-function}{{1.3.4}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.5}Regularization}{9}}
